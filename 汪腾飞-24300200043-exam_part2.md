# 汪腾飞-24300200043-exam_part2

## 第一题
    import numpy as np
    def matrix_stats(mat):
        arr = np.array(mat)
        if np.std(arr)!=0:
            return {
                'max': np.max(arr),
                'min': np.min(arr),
                'mean': np.mean(arr),
                'std': np.std(arr),
                'sr': np.mean(arr) / np.std(arr),
                'sum': np.sum(arr)
            }
        else:
            return {
                'max': np.max(arr),
                'min': np.min(arr),
                'mean': np.mean(arr),
                'std': '无',
                'sr': '无',
                'sum': np.sum(arr)
            }

    if __name__ == '__main__':
        A=[[1,2,3,],[4,5,6]]
        B=[[-1,0],[0,1]]

        max_A,min_A,mean_A,std_A,sr_A,sum_A= matrix_stats(A).values()
        max_B,min_B,mean_B,std_B,sr_B,sum_B= matrix_stats(B).values()
        print(max_A,min_A,mean_A,std_A,sr_A,sum_A,sep='\t')
        print(max_B,min_B,mean_B,std_B,sr_B,sum_B,sep='\t')

        taxt={1:[[1,0,5],[9,1,5],[5,5,5]] ,2:[[2,5,7,5],[84,64,84,84]] ,3:[[0,0],[0,0]] ,4:[[1,0],[0,1]] ,5:[[5,5],[5,5]] }
        for i in taxt.keys():
            max_A,min_A,mean_A,std_A,sr_A,sum_A= matrix_stats(taxt[i]).values()
            
            print(i)
            print(max_A,min_A,mean_A,std_A,sr_A,sum_A,sep='\t')
结果如下:![1](https://github.com/jiji-yi-qiangqiang/exam/blob/main/第一题.png)

## 第二题
    R = 3;
    r = 0.7;
    u = -r:0.014:r;
    v = 0:pi/50:2*pi;

    % 创建网格
    [U, V] = meshgrid(u, v);

    % 使用网格坐标计算
    x = (R + U .* cos(V/2)) .* cos(V);
    y = (R + U .* cos(V/2)) .* sin(V);
    z = U .* sin(V/2);

    surf(x, y, z);
    title('Möbius Strip');
    xlabel('X');
    ylabel('Y');
    zlabel('Z');
    axis equal;
    shading interp;
    colorbar;
结果如下:![2](https://github.com/jiji-yi-qiangqiang/exam/blob/main/第二题.png)

## 第三题
    NSum[Cos[Pi/n]/n^3, {n, 1, Infinity}]
    NIntegrate[Sin[x]/(x (Exp[x] + 1)), {x, 0, Infinity}]

## 第四题
# Linear Least Squares
Linear least squares (LLS) is the least squares approximation of linear functions to data. It is a set of formulations for solving statistical problems involved in [linear regression](),including variants for ordinary (unweighted), weighted, and generalized (correlated) residuals. Numerical methods for linear least squares include inverting the matrix of the normal equations and orthogonal decomposition methods.

## Basic Formulation
Consider the linear equation $$Ax = b$$ where $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^m$ are given and $x \in \mathbb{R}^n$ is variable to be computed. When $m > n$ ,it is generally the case that Eq. (1) has no solution.For example, there is no value of that satisfies $$[1,0;0,1;1,1]*x=[1;1;0]$$ because the first two rows require that $x=(1,1)$, but then the third row is not satisfied.Thus,for $m>n$,the goal of solving Eq. (1) exactly is typically replaced by finding the value of that minimizes some error. There are many ways that the error can be defined, but one of the most common is to define it as $||Ax-b||^2$.This produces a minimization problem, called a least squares problem $$minimize_{x \in \mathbb{R}^n}||Ax-b||^2$$. The solution to the least squares problem is computed by solving the normal equation $$A^TAx=A^Tb$$ while $A^T$ denotes the transpose of the matrix $A$ .